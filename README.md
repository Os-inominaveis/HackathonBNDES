Repositório da equipe *Os Inomináveis* para o Prêmio Dados Abertos para o Desenvolvimento do BNDES

*Instruções: Siga o passo a passo abaixo para compreender e conseguir executar o código em sua máquina, Obrigado.*

# HackathonBNDES

O Prêmio Dados Abertos para o Desenvolvimento é uma iniciativa do BNDES para incentivar a colaboração entre governo e sociedade e para estimular o desenvolvimento de estudos, análises e soluções de empreendedorismo tecnológico que contribuam para aprimorar e modernizar a gestão pública.

Com essa iniciativa, pretende-se trazer ganhos à administração pública, com ampliação da participação e controle social e contribuições para a gestão decorrentes da análise dos dados abertos; além de ganhos para a sociedade, por meio da melhoria da gestão pública e do serviço público como um todo.

O Plano de Dados Abertos do BNDES estabelece a governança e a sistemática de publicação e atualização dos dados, além de um cronograma para a abertura das bases do Banco.

#Este projeto é uma solução desenvolvida específicamente para o BNDES como proposta para o desafio. Todos os dados utilizados estão disponíveis em bases abertas e seguem o protocolo de segurança de não identificação de usuários. [Fonte](https://dadosabertos.bndes.gov.br/dataset/operacoes-financiamento)

Como método de avaliaçao, segue como guia os seguintes parâmetros

# Critérios:

a) Impacto: a utilização da aplicação tem impacto relevante na construção do conhecimento, no entendimento e na promoção de iniciativas de gestão de recursos ou de negócio do BNDES, contribuindo, ainda, para a transparência de sua atuação.

b) Usabilidade: a solução se mostra uma ferramenta de fácil compreensão e aplicação pelo analista de negócio do BNDES.

c) Criatividade e originalidade: solução diferenciada, agregando algo de novo ao repertório de análises realizadas pelo BNDES, acrescentando valor às ações do Banco ou transparência para a sociedade.

d) Qualidade técnica: a análise fez uso de técnicas avançadas ou incomuns. Além do uso das funcionalidades prontas, a análise foi sofisticada com o uso de codificação, modelagem matemática, machine learning ou mineração de dados.

# Tema escolhido
Operações contratadas na forma indireta automática
Operações de financiamento contratadas e que foram analisadas pelas instituições financeiras credenciadas, no âmbito dosprodutos: BNDES FINAME e BNDES Automático.

#### -- Status do Projeto: [Finalizado - Entregue]

## Descrição do projeto

*Com base no problema observado, é de suma importância trazer mais segurança ao agente financeiro de que o risco de crédito da linha de financiamento seja mínimo.
E se pudéssemos saber antecipadamente se um empréstimo será pago?Quais seriam os impactos em ganhos?
Quais seriam os impactos em ganhos?*

## Introduçã/Objetivo

A proposta do projeto envolve 4 pontos, com o objetivo de predição de clientes inadimplentes.

1- Análise descritiva dos dados;
2- Identificação de padrão de inadimplência nos financiamentos;
3- Características mais relevantes;
4- Predição de liquidez no prazo definido.

### Parceiro
* [BNDES]
* [Website](https://www.bndes.gov.br/wps/portal/site/home/transparencia/iniciativas/!ut/p/z1/fY5NC4JAEIbv_govHmVWiOxqFn4QgeRB9yKTLjGVu-pu0s_PxOrWZZ53YJ6XAW7ZtmVBMWEKy_gQuMSRLmhISbxDASVfV34WhXGw8g6bPA5ZFqbT4kdelHiQzuLPfzf8N4rFWM6_gJKufc8D4LWSRjwNFNvjbn-qEqkNmUc9f-SwWLXCYfmAUnc4CFkTOowkTTQ0onZYN4iWlNtgo7SLZzEYpaG78fIFRBWnjA!!/)

### Métodos Usados
* Estatística Inferencial;
* Visualização de dados;
* Machine Learning;
* Modelo de Predição.

### Technologies

* Python;
* Pandas, Jupyter;
* Seaborn, Matplotlib;
* Profilling;
* Requests, Pathlib
* Datetime
* Sklearn
* Lightgbm
* Yellowbricks

## Necessidades do projeto

- Exploração de dados / Estatística Descritiva
- Processamento de dados / Limpeza
- Modelagem Estatística
- Storytelling/Relatórios


## Iniciando

1. Clone this repo (for help see this [tutorial](https://help.github.com/articles/cloning-a-repository/)).
2. Raw Data is being kept [here](Repo folder containing raw data) within this repo.

    *If using offline data mention that and how they may obtain the data from the froup)*
    
3. Data processing/transformation scripts are being kept [here](Repo folder containing data processing scripts/notebooks)
4. etc...

*If your project is well underway and setup is fairly complicated (ie. requires installation of many packages) create another "setup.md" file and link to it here*  

5. Follow setup [instructions](Link to file)

## Featured Notebooks/Analysis/Deliverables
* [Notebook/Markdown/Slide Deck Title](link)
* [Notebook/Markdown/Slide DeckTitle](link)
* [Blog Post](link)


## Contributing DSWG Members

**Team Leads (Contacts) : [Full Name](https://github.com/[github handle])(@slackHandle)**

#### Other Members:

|Name     |  Slack Handle   | 
|---------|-----------------|
|[Full Name](https://github.com/[github handle])| @johnDoe        |
|[Full Name](https://github.com/[github handle]) |     @janeDoe    |

## Contact
* If you haven't joined the SF Brigade Slack, [you can do that here](http://c4sf.me/slack).  
* Our slack channel is `#datasci-projectname`
* Feel free to contact team leads with any questions or if you are interested in contributing!
This project is a part of the [Data Science Working Group](http://datascience.codeforsanfrancisco.org) at [Code for San Francisco](http://www.codeforsanfrancisco.org).  Other DSWG projects can be found at the [main GitHub repo](https://github.com/sfbrigade/data-science-wg).

#### -- Project Status: [Active, On-Hold, Completed]

## Project Intro/Objective
The purpose of this project is ________. (Describe the main goals of the project and potential civic impact. Limit to a short paragraph, 3-6 Sentences)

### Partner
* [Name of Partner organization/Government department etc..]
* Website for partner
* Partner contact: [Name of Contact], [slack handle of contact if any]
* If you do not have a partner leave this section out

### Methods Used
* Inferential Statistics
* Machine Learning
* Data Visualization
* Predictive Modeling
* etc.

### Technologies
* R 
* Python
* D3
* PostGres, MySql
* Pandas, jupyter
* HTML
* JavaScript
* etc. 

## Project Description
(Provide more detailed overview of the project.  Talk a bit about your data sources and what questions and hypothesis you are exploring. What specific data analysis/visualization and modelling work are you using to solve the problem? What blockers and challenges are you facing?  Feel free to number or bullet point things here)

## Needs of this project

- frontend developers
- data exploration/descriptive statistics
- data processing/cleaning
- statistical modeling
- writeup/reporting
- etc. (be as specific as possible)

## Getting Started

1. Clone this repo (for help see this [tutorial](https://help.github.com/articles/cloning-a-repository/)).
2. Raw Data is being kept [here](Repo folder containing raw data) within this repo.

    *If using offline data mention that and how they may obtain the data from the froup)*
    
3. Data processing/transformation scripts are being kept [here](Repo folder containing data processing scripts/notebooks)
4. etc...

*If your project is well underway and setup is fairly complicated (ie. requires installation of many packages) create another "setup.md" file and link to it here*  

5. Follow setup [instructions](Link to file)

## Featured Notebooks/Analysis/Deliverables
* [Notebook/Markdown/Slide Deck Title](link)
* [Notebook/Markdown/Slide DeckTitle](link)
* [Blog Post](link)


## Contributing DSWG Members

**Team Leads (Contacts) : [Full Name](https://github.com/[github handle])(@slackHandle)**

#### Other Members:

|Name     |  Slack Handle   | 
|---------|-----------------|
|[Full Name](https://github.com/[github handle])| @johnDoe        |
|[Full Name](https://github.com/[github handle]) |     @janeDoe    |

```
├── .gitignore               <- Files that should be ignored by git. Add seperate .gitignore files in sub folders if 
│                               needed
├── conda_env.yml            <- Conda environment definition for ensuring consistent setup across environments
├── LICENSE
├── README.md                <- The top-level README for developers using this project.
├── requirements.txt         <- The requirements file for reproducing the analysis environment, e.g.
│                               generated with `pip freeze > requirements.txt`. Might not be needed if using conda.
├── setup.py                 <- Metadata about your project for easy distribution.
│
├── data
│   ├── interim_[desc]       <- Interim files - give these folders whatever name makes sense.
│   ├── processed            <- The final, canonical data sets for modeling.
│   ├── raw                  <- The original, immutable data dump.
│   ├── temp                 <- Temporary files.
│   └── training             <- Files relating to the training process
│
├── docs                     <- Documentation
│   ├── data_science_code_of_conduct.md  <- Code of conduct.
│   ├── process_documentation.md         <- Standard template for documenting process and decisions.
│   └── writeup              <- Sphinx project for project writeup including auto generated API.
│      ├── conf.py           <- Sphinx configurtation file.
│      ├── index.rst         <- Start page.
│      ├── make.bat          <- For generating documentation (Windows)
│      └── Makefikle         <- For generating documentation (make)
│
├── examples                 <- Add folders as needed e.g. examples, eda, use case
│
├── extras                   <- Miscellaneous extras.
│   └── add_explorer_context_shortcuts.reg    <- Adds additional Windows Explorer context menus for starting jupyter.
│
├── notebooks                <- Notebooks for analysis and testing
│   ├── eda                  <- Notebooks for EDA
│   │   └── example.ipynb    <- Example python notebook
│   ├── features             <- Notebooks for generating and analysing features (1 per feature)
│   ├── modelling            <- Notebooks for modelling
│   └── preprocessing        <- Notebooks for Preprocessing 
│
├── scripts                  <- Standalone scripts
│   ├── deploy               <- MLOps scripts for deployment (WIP)
│   │   └── score.py         <- Scoring script
│   ├── train                <- MLOps scripts for training
│   │   ├── submit-train.py  <- Script for submitting a training run to Azure ML Service
│   │   ├── submit-train-local.py <- Script for local training using Azure ML
│   │   └── train.py         <- Example training script using the iris dataset
│   ├── example.py           <- Example sctipt
│   └── MLOps.ipynb          <- End to end MLOps example (To be refactored into the above)
│
├── src                      <- Code for use in this project.
│   └── examplepackage       <- Example python package - place shared code in such a package
│       ├── __init__.py      <- Python package initialisation
│       ├── examplemodule.py <- Example module with functions and naming / commenting best practices
│       ├── features.py      <- Feature engineering functionality
│       ├── io.py            <- IO functionality
│       └── pipeline.py      <- Pipeline functionality
│
└── tests                    <- Test cases (named after module)
    ├── test_notebook.py     <- Example testing that Jupyter notebooks run without errors
    ├── examplepackage       <- examplepackage tests
        ├── examplemodule    <- examplemodule tests (1 file per method tested)
        ├── features         <- features tests
        ├── io               <- io tests
        └── pipeline         <- pipeline tests
```
## Contact
* If you haven't joined the SF Brigade Slack, [you can do that here](http://c4sf.me/slack).  
* Our slack channel is `#datasci-projectname`
* Feel free to contact team leads with any questions or if you are interested in contributing!
